---
title: "ML_Proj8 - Final"
author: "Austin Marckx"
date: "`r Sys.Date()`"
output:
  word_document: default
  pdf_document: default
  html_document: default
---

```{r setup, include=FALSE}
library(GGally)
library(ggpubr)
library(readxl)
library(randomForest)
library(caret)
library(cowplot)
library(tidyverse)
library(ggridges)
library(boot)
library(mclust)
library(corrplot)
library("FactoMineR")
library("factoextra")
library(nlme)
library(lme4)
library("REEMtree")
library(brms)
library(rstatix)

minmax <- function(x){
  return( (x - min(x))/(max(x) - min(x))  )
}

set.seed(3141526)
```


# Basic Preprocessing

- Read in data
- Ensure correct column types
- Check for missing data


```{r}
# Read in file
df <- read_xlsx('./data/AD.training.xlsx')

# Transform datatypes
df <- df %>% transform(
  PTID = factor(PTID),
  DX = factor(DX.bl, levels = c("CN", "EMCI", "LMCI", "AD")),
  PTGENDER = factor(PTGENDER),
  APOE4 = factor(APOE4)
) %>% select(AGE, PTEDUCAT, MMSE, PTGENDER, APOE4, DX, everything(), -DX.bl)

# Summary
df %>% head()
df %>% str()
df %>% summary()

any(is.na(df))
df[!complete.cases(df),]

```

There do not appear to be any missing values. Moreover each patient appears to be only represented once in the dataset (as the number of unique patient ids == number of rows.)


### Pair plot

- Looking for highly skewed variables
- Features which are collinear/linear transforms of one another/highly redundant
- Diversity sampling issues

```{r, fig.width= 18, fig.height=18, message = FALSE, warning = FALSE}
df %>% 
  mutate(asinsqrtMMSE = asin(sqrt(minmax(MMSE)))  ) %>%
  select(AGE, PTEDUCAT, MMSE, asinsqrtMMSE, everything(), MMSE.Change, -RID, -PTID, -EXAMDATE) %>% # removing factors with too many levels or irrelevant features
  ggpairs(aes(fill = PTGENDER, alpha = 0.7), progress = FALSE)

```
#
```{r}
# https://www.datanovia.com/en/blog/elegant-visualization-of-density-distribution-in-r-using-ridgeline/
df %>%
  ggplot(aes(x = `MMSE.Change`, y = DX, group = DX, fill = factor(stat(quantile)))) + 
  stat_density_ridges(
    geom = "density_ridges_gradient",
    calc_ecdf = TRUE,
    quantiles = 4,
    quantile_lines = TRUE, 
    jittered_points = TRUE,
    scale = 0.9,
    position = position_points_jitter(width = 0.05, height = 0.1),
    point_size = 1, point_alpha = 0.5, alpha = 0.7) + 
  theme_minimal() +
  scale_fill_brewer(name = 'Quartiles')
```

```{r, fig.width= 18, fig.height=18, message = FALSE, warning = FALSE}
df %>% 
  #mutate(asinsqrtMMSE = asin(sqrt(minmax(MMSE)))  ) %>%
  select(AGE, PTEDUCAT, MMSE, everything(), MMSE.Change, -RID, -PTID, -EXAMDATE) %>% # removing factors with too many levels or irrelevant features
  ggpairs(aes(fill = DX, alpha = 0.7), progress = FALSE)

```

```{r, fig.width= 12, fig.height=12, message = FALSE, warning = FALSE}
df %>% 
  mutate(asinsqrtMMSE = asin(sqrt(minmax(MMSE))), logMMSE = log(MMSE+1), MMSE4mo = MMSE + `MMSE.Change`, logMMSE4mo = log(MMSE4mo+1), asinsqrtMMSE4mo = asin(sqrt(minmax(MMSE4mo)))) %>%
  select(MMSE, asinsqrtMMSE, logMMSE, MMSE.Change, MMSE4mo, logMMSE4mo, asinsqrtMMSE4mo, DX) %>% # removing factors with too many levels or irrelevant features
  ggpairs(aes(fill = DX, alpha = 0.7), progress = FALSE)

```

#
```{r}
# https://www.datanovia.com/en/blog/elegant-visualization-of-density-distribution-in-r-using-ridgeline/
df %>%
  select(PTID, `MMSE.Change`, MMSE, DX) %>%
  mutate(MMSE4mo = MMSE + `MMSE.Change`) %>%
  select(-`MMSE.Change`) %>%
  pivot_longer(cols = c(MMSE4mo, MMSE),  names_to = 'time', values_to = 'MMSE') %>%
  ggplot(aes(x = MMSE, y = time, group = time, fill = factor(stat(quantile)))) + 
  stat_density_ridges(
    geom = "density_ridges_gradient",
    calc_ecdf = TRUE,
    quantiles = 4,
    quantile_lines = TRUE, 
    jittered_points = TRUE,
    scale = 0.9,
    position = position_points_jitter(width = 0.05, height = 0.1),
    point_size = 1, point_alpha = 0.5, alpha = 0.7) + 
  theme_minimal() +
  scale_fill_brewer(name = 'Quartiles')
```

#
```{r}
# https://www.datanovia.com/en/blog/elegant-visualization-of-density-distribution-in-r-using-ridgeline/
df %>%
  select(PTID, `MMSE.Change`, MMSE, DX) %>%
  mutate(MMSE4mo = MMSE + `MMSE.Change`) %>%
  select(-`MMSE.Change`) %>%
  pivot_longer(cols = c(MMSE4mo, MMSE),  names_to = 'time', values_to = 'MMSE') %>%
  ggplot(aes(x = MMSE, y = time, group = PTID, color = DX)) + 
    geom_jitter(width = 0.2, height = 0.075) +
    geom_line() +
    geom_violin(aes(color = NULL, fill = time, group = time), alpha = 0.15, draw_quantiles = c(0.25, 0.5, 0.75)) + 
  theme_minimal() +
  facet_wrap(~DX)
```

# Look at Summary stats
```{r}

# By Time
df %>% 
  select(PTID, `MMSE.Change`, MMSE, DX) %>%
  mutate(MMSE4mo = MMSE + `MMSE.Change`) %>%
  select(-`MMSE.Change`) %>%
  pivot_longer(cols = c(MMSE4mo,MMSE),  names_to = 'time', values_to = 'MMSE') %>%
  group_by(time) %>% rstatix::get_summary_stats()

# By time and DX
df %>% 
  select(PTID, `MMSE.Change`, MMSE, DX) %>%
  mutate(MMSE4mo = MMSE + `MMSE.Change`) %>%
  select(-`MMSE.Change`) %>%
  pivot_longer(cols = c(MMSE4mo,MMSE),  names_to = 'time', values_to = 'MMSE') %>%
  group_by(DX, time) %>% rstatix::get_summary_stats()

```


salmon - CN
green - EMCI (Early mild cognitive impairment)
blue - LMCI (Late mild cognitive impairment)
purple - AD

# Remove non predictive features
```{r}
# Subset to only useful features
df_clean <- df %>% select(-RID, -PTID, -EXAMDATE)
df_clean %>% head()

# Optional
df_clean <- df_clean #%>% mutate(`MMSE.Change` = ifelse(MMSE.Change < -8, -8, MMSE.Change))
# Remove non predictive features
#identify_outliers(df_clean %>% dplyr::select(MMSE.Change))


df_preds <- df %>% select(PTID, MMSE.Change)# %>% mutate(`MMSE.Change` = ifelse(MMSE.Change < -8, -8, MMSE.Change))

# More subsets
X <- df_clean %>% select(-`MMSE.Change`)
df_numerical <- df_clean %>% select(-`MMSE.Change`, -PTGENDER, -APOE4, -DX)
df_categorical <- df_clean %>% select(-`MMSE.Change`, -AGE, -MMSE, -PTEDUCAT)
```

```{r}
# Subset to only useful features
df_lme <- df %>% select(-RID, -EXAMDATE) #%>% mutate(`MMSE.Change` = ifelse(MMSE.Change < -8, -8, MMSE.Change))
df_lme %>% head()

# More subsets
X_lme <- df_lme %>% select(-`MMSE.Change`)
```


# GLM Regression

## gaussian
```{r}
gauss <- glm(`MMSE.Change` ~ ., data = df_clean, family = gaussian())
coef(summary(gauss))[,4]
summary(gauss)
# RMSE
sqrt(mean(gauss$residuals ^ 2))
plot(gauss)

df_preds$glm <- predict(gauss)
df_preds %>%
  ggplot(aes(x = MMSE.Change, y = glm)) +
  geom_point(alpha = 0.7) +
  geom_abline(slope = 1, color = "red") +
  theme_minimal()

df_preds %>%
  ggplot(aes(x = MMSE.Change, y = round(glm, 0)) ) +
  geom_point() +
  geom_abline(slope = 1, color = "red")


```


# Random Forest Regression: 
```{r}
rf <- randomForest(`MMSE.Change` ~ ., data = df_clean, ntrees = 500, importance = TRUE, type = 'regression')
df_preds$rf <- predict(rf)

# RMSE
sqrt(mean(rf$mse))

rf
plot(rf)
importance(rf)
varImpPlot(rf)

df_preds %>%
  ggplot(aes(x = MMSE.Change, y = rf)) +
  geom_point(alpha = 0.7) +
  geom_abline(slope = 1, color = "red") +
  theme_minimal()

df_preds %>%
  ggplot(aes(x = MMSE.Change, y = round(rf, 0)) ) +
  geom_point() +
  geom_abline(slope = 1, color = "red")

```

# Linear Mixed Effects



```{r}
train <- df_lme %>% dplyr::sample_frac(0.70)
test  <- dplyr::anti_join(df_lme, train, by = 'PTID')

df_lme
train
test
```


```{r}
lmeModtt <- lme(`MMSE.Change`~ AGE+MMSE+PTGENDER+PTEDUCAT+APOE4, random = ~ 1|DX, data = train)
lmeModtt
print("RMSE: ")
sqrt(mean(lmeModtt$residuals ^ 2))
summary(lmeModtt)

plot(lmeModtt)

train %>%
  ggplot(aes(x = MMSE.Change, y = predict(lmeModtt), color = DX)) +
  geom_point(alpha = 0.7) +
  geom_abline(slope = 1, color = "red") +
  theme_minimal()

test$ypred = predict(lmeModtt, newdata = test)

test


test %>%
  ggplot(aes(x = MMSE.Change, y = predict(lmeModtt, newdata = test), color = DX)) +
  geom_point(alpha = 0.7) +
  geom_abline(slope = 1, color = "red") +
  theme_minimal()

plot(ranef(lmeModtt))
```


```{r}
lmeModtt2 <- lme(`MMSE.Change`~ AGE+MMSE+PTGENDER+PTEDUCAT+APOE4, random = ~ 1|PTID, data = train)
lmeModtt2
print("RMSE: ")
sqrt(mean(lmeModtt2$residuals ^ 2))
summary(lmeModtt2)

plot(lmeModtt2)

train %>%
  ggplot(aes(x = MMSE.Change, y = predict(lmeModtt2), color = DX)) +
  geom_point(alpha = 0.7) +
  geom_abline(slope = 1, color = "red") +
  theme_minimal()

predict(lmeModtt2, newdata = test, allow.new.levels = TRUE)

test %>%
  ggplot(aes(x = MMSE.Change, y = predict(lmeModtt2, newdata = test), color = DX)) +
  geom_point(alpha = 0.7) +
  geom_abline(slope = 1, color = "red") +
  theme_minimal()

plot(ranef(lmeModtt2))
```






















```{r}
lmeMod <- lme(`MMSE.Change`~ AGE+MMSE+PTGENDER+PTEDUCAT+APOE4+DX*MMSE, random = ~ 1|PTID, data = df_lme)
lmeMod
print("RMSE: ")
sqrt(mean(lmeMod$residuals ^ 2))
summary(lmeMod)

# RMSE
df_preds$lme <- predict(lmeMod)

plot(lmeMod)

df_preds %>%
  ggplot(aes(x = MMSE.Change, y = lme)) +
  geom_point(alpha = 0.7) +
  geom_abline(slope = 1, color = "red") +
  theme_minimal()

plot(ranef(lmeMod))
```


```{r}
df_preds %>%
  ggplot(aes(x = MMSE.Change, y = lme)) +
  geom_point(alpha = 0.7) +
  geom_abline(slope = 1, color = "red") +
  theme_minimal()

df_preds %>%
  ggplot(aes(x = MMSE.Change, y = round(lme, 0)) ) +
  geom_point() +
  geom_abline(slope = 1, color = "red")

df_preds %>%
  ggplot(aes(x = MMSE.Change, y = lme)) +
  geom_jitter(height = 0, width = 0.1, alpha = 0.7) +
  geom_abline(slope = 1, color = "red")



```


```{r}
lmeMod2 <- lme(`MMSE.Change`~ AGE+PTGENDER+PTEDUCAT+APOE4+DX*MMSE, random = ~ 1|PTID, data = df_lme)
lmeMod2
print("RMSE: ")
sqrt(mean(lmeMod2$residuals ^ 2))
summary(lmeMod2)

# RMSE
df_lme$lme2_yPred <- predict(lmeMod2)

plot(lmeMod2)
```


```{r}
df_lme %>%
  ggplot(aes(x = MMSE.Change, y = lme2_yPred)) +
  geom_point(alpha = 0.7) +
  geom_abline(slope = 1, color = "red")

df_lme %>%
  ggplot(aes(x = MMSE.Change, y = round(lme2_yPred, 0)) ) +
  geom_point() +
  geom_abline(slope = 1, color = "red")


```


# Bayesian Mixed Effects Regression: 
```{r}
bme <- brm(MMSE.Change ~ AGE+MMSE+PTGENDER+PTEDUCAT+APOE4+DX + (1 + DX|PTID), data = df_lme)
df_preds$bme <- predict(bme)
summary(bme)
```

-- it seems like there is a small improvement in performance after increasing the num of iterations

Family: gaussian 
  Links: mu = identity; sigma = identity 
Formula: MMSE.Change ~ AGE + MMSE + PTGENDER + PTEDUCAT + APOE4 + DX + (1 + DX | PTID) 
   Data: df_lme (Number of observations: 384) 
  Draws: 4 chains, each with iter = 3000; warmup = 1500; thin = 1;
         total post-warmup draws = 6000

Group-Level Effects: 
~PTID (Number of levels: 384) 
                      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
sd(Intercept)             0.74      0.37     0.04     1.31 1.08       36       42
sd(DXEMCI)                1.30      0.41     0.52     2.09 1.05       86      257
sd(DXLMCI)                3.07      0.41     2.31     3.94 1.08       55      269
sd(DXAD)                  5.26      0.62     4.13     6.54 1.02      232      777
cor(Intercept,DXEMCI)     0.05      0.43    -0.75     0.82 1.03      157      436
cor(Intercept,DXLMCI)     0.19      0.42    -0.67     0.87 1.12       26      175
cor(DXEMCI,DXLMCI)        0.02      0.42    -0.78     0.80 1.03      120      372
cor(Intercept,DXAD)       0.07      0.43    -0.78     0.80 1.01      187      356
cor(DXEMCI,DXAD)          0.02      0.47    -0.85     0.84 1.03      156      416
cor(DXLMCI,DXAD)         -0.01      0.45    -0.82     0.79 1.03      169      640

Population-Level Effects: 
             Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
Intercept       13.10      2.74     7.69    18.38 1.00     1510     3600
AGE              0.00      0.02    -0.03     0.04 1.00     2866     3334
MMSE            -0.48      0.08    -0.64    -0.32 1.01      952     3324
PTGENDERMale     0.07      0.20    -0.34     0.46 1.01     2989     2519
PTEDUCAT         0.03      0.04    -0.05     0.11 1.00     2704     3190
APOE41          -0.09      0.23    -0.55     0.34 1.00     1104     3568
APOE42          -0.56      0.44    -1.44     0.26 1.02      248       54
DXEMCI          -0.58      0.29    -1.15    -0.02 1.01     2680     3806
DXLMCI          -2.49      0.37    -3.21    -1.77 1.00     2261     3274
DXAD            -7.15      0.81    -8.76    -5.53 1.00     1797     2783

Family Specific Parameters: 
      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
sigma     0.95      0.28     0.39     1.38 1.10       31       29

Draws were sampled using sampling(NUTS). For each parameter, Bulk_ESS
and Tail_ESS are effective sample size measures, and Rhat is the potential
scale reduction factor on split chains (at convergence, Rhat = 1)



```{r}
df_preds %>% str()
df_preds$bme <- df_preds$bme[,1]

#df_preds <- df_preds %>% select(-bmepreds)
```

```{r}
df_preds %>% 
  mutate(lme_rounded = round(lme, 0)) %>%
  pivot_longer(cols = c(glm, rf, lme, lme_rounded, bme), names_to = "model", values_to = "ypred") %>%
  mutate(predresidual = MMSE.Change - ypred, model = factor(model, levels = c("rf", "glm", "lme", "lme_rounded", "bme")) ) %>%
  ggplot(aes(x = model, y = sqrt(abs(predresidual)), color = model)) + 
    geom_jitter(width = 0.3, height = 0.035, alpha = 0.7) +
    stat_summary(fun.data = mean_se, geom = "errorbar", color = "black", width= 0.2) +
    theme_minimal()


df_preds %>% 
  mutate(lme_rounded = round(lme, 0)) %>%
  pivot_longer(cols = c(glm, rf, lme, lme_rounded, bme), names_to = "model", values_to = "ypred") %>%
  mutate(model = factor(model, levels = c("rf", "glm", "lme", "lme_rounded", "bme")) ) %>%
  ggplot(aes(x = MMSE.Change, y = ypred, color = model)) + 
    geom_point(alpha = 0.7) +
    geom_abline(slope = 1, color = "red") +
    theme_minimal() +
    facet_wrap(~model)

```


```{r}
df_preds %>% 
  mutate(lme_rounded = round(lme, 0)) %>%
  pivot_longer(cols = c(glm, rf, lme, lme_rounded, bme), names_to = "model", values_to = "ypred") %>%
  mutate(model = factor(model, levels = c("rf", "glm", "lme", "lme_rounded", "bme")) ) %>%
  ggplot(aes(x = MMSE.Change, y = ypred)) + 
    stat_binhex(alpha = 0.7, binwidth = c(0.5, 0.5)) +
    geom_abline(slope = 1, color = "red") +
    theme_minimal() +
    facet_wrap(~model)



df_preds %>% 
  #mutate(lme_rounded = round(lme, 0)) %>%
  pivot_longer(cols = c(glm, rf, lme, bme), names_to = "model", values_to = "ypred") %>%
  mutate(ypred = round(ypred, 0)) %>%
  mutate(model = factor(model, levels = c("rf", "glm", "lme", "bme")) ) %>%
  ggplot(aes(x = MMSE.Change, y = ypred, color = model)) + 
    geom_count(alpha = 0.7, aes(size = after_stat(prop))) +
    geom_abline(slope = 1, color = "red") +
    theme_minimal() +
    facet_wrap(~model)
```


```{r}
pp_check(bme, ndraws = 30)
#https://towardsdatascience.com/evaluating-bayesian-mixed-models-in-r-python-27d344a03016
#http://mjskay.github.io/tidybayes/
#https://htmlpreview.github.io/?https://github.com/ecoronado92/towards_data_science/blob/master/hierarchical_models/bayes_lmm/loo_pit_example/loo_pit.html
```



# Bayesian Mixed Effects Regression: 
```{r}
#bme2 <- brm(MMSE.Change ~ AGE+MMSE+PTGENDER+PTEDUCAT+APOE4+DX + (1 + DX|PTID), data = df_lme, iter = 5000)
df_preds$bme2 <- predict(bme2)
summary(bme2)
df_preds$bme2 <- df_preds$bme2[,1]

df_preds %>% 
  mutate(lme_rounded = round(lme, 0)) %>%
  pivot_longer(cols = c(glm, rf, lme, lme_rounded, bme), names_to = "model", values_to = "ypred") %>%
  mutate(predresidual = MMSE.Change - ypred, model = factor(model, levels = c("rf", "glm", "lme", "lme_rounded", "bme", "bme2")) ) %>%
  ggplot(aes(x = model, y = sqrt(abs(predresidual)), color = model)) + 
    geom_jitter(width = 0.3, height = 0.035, alpha = 0.7) +
    stat_summary(fun.data = mean_se, geom = "errorbar", color = "black", width= 0.2) +
    theme_minimal()


df_preds %>% 
  mutate(lme_rounded = round(lme, 0)) %>%
  pivot_longer(cols = c(glm, rf, lme, lme_rounded, bme), names_to = "model", values_to = "ypred") %>%
  mutate(model = factor(model, levels = c("rf", "glm", "lme", "lme_rounded", "bme",  "bme2")) ) %>%
  ggplot(aes(x = MMSE.Change, y = ypred, color = model)) + 
    geom_point(alpha = 0.7) +
    geom_abline(slope = 1, color = "red") +
    theme_minimal() +
    facet_wrap(~model)
```


```{r}
pp_check(bme2, ndraws = 30)
#https://towardsdatascience.com/evaluating-bayesian-mixed-models-in-r-python-27d344a03016
#http://mjskay.github.io/tidybayes/
#https://htmlpreview.github.io/?https://github.com/ecoronado92/towards_data_science/blob/master/hierarchical_models/bayes_lmm/loo_pit_example/loo_pit.html
```

# PCA 
```{r}
#http://www.sthda.com/english/articles/31-principal-component-methods-in-r-practical-guide/112-pca-principal-component-analysis-essentials/

resPCA <- PCA(df_numerical) # Note, this automatically scales the data

resPCA$eig
fviz_screeplot(resPCA)
corrplot(resPCA$var$contrib, is.corr = FALSE, col = COL1('YlGn', 20))


fviz_pca_ind(resPCA, col.ind = "contrib", 
             gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"),
             repel = TRUE)

fviz_pca_var(resPCA, col.var = "contrib", 
             gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"),
             repel = TRUE)

```



# FAMD
```{r}
# http://www.sthda.com/english/articles/31-principal-component-methods-in-r-practical-guide/115-famd-factor-analysis-of-mixed-data-in-r-essentials/

resFAMD <- FAMD(X)

resFAMD$eig
corrplot(resFAMD$var$contrib, is.corr = FALSE, col = COL1('YlGn', 20))
fviz_famd_var(resFAMD,"var",
             repel = TRUE)

fviz_mfa_ind(resFAMD, 
             habillage = "DX", # color by groups 
             #palette = c("#00AFBB", "#E7B800", "#FC4E07"),
             addEllipses = TRUE, ellipse.type = "confidence", 
             repel = TRUE # Avoid text overlapping
             ) 
```

# Bootstrapping (!?): 
```{r, eval = FALSE}
#https://www.statmethods.net/advstats/bootstrapping.html
bs <- function(formula, data, indices) {
  d <- data[indices,] # allows boot to select sample
  fit <- glm(formula, data=d, family = gaussian())
  return(coef(fit))
}

results <- boot(data = df_clean, statistic=bs, formula = `MMSE.Change` ~ ., R = 1000)

results
plot(results)
plot(results, index = 10)

boot.ci(results, index = 10)
```


# Gaussian Mixture: 
```{r,eval = FALSE, fig.height = 12, fig.width=12}
gmm <- Mclust(df_numerical)
summary(gmm)
plot(gmm, what = "BIC")
plot(gmm, what = "classification")


```

```{r}
sessionInfo()
```



