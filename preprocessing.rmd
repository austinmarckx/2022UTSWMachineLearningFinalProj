---
title: "ML_Proj3"
author: "Austin Marckx"
date: "`r Sys.Date()`"
output:
  word_document: default
  pdf_document: default
  html_document: default
---

```{r setup, include=FALSE}
library(GGally)
library(ggpubr)
library(readxl)
library(randomForest)
library(caret)
library(cowplot)
library(tidyverse)

minmax <- function(x){
  return( (x - min(x))/(max(x) - min(x))  )
}

set.seed(3141526)
```


# Basic Preprocessing

- Read in data
- Ensure correct column types
- Check for missing data


```{r}
# Read in file
df <- read_xlsx('./data/AD.training.xlsx')

# Transform datatypes
df <- df %>% transform(
  PTID = factor(PTID),
  DX = factor(DX.bl),
  PTGENDER = factor(PTGENDER),
  APOE4 = factor(APOE4)
) %>% select(AGE, PTEDUCAT, MMSE, PTGENDER, APOE4, DX, everything(), -DX.bl)

# Summary
df %>% head()
df %>% str()
df %>% summary()

any(is.na(df))
df[!complete.cases(df),]

```

There do not appear to be any missing values. Moreover each patient appears to be only represented once in the dataset (as the number of unique patient ids == number of rows.)


### Pair plot

- Looking for highly skewed variables
- Features which are collinear/linear transforms of one another/highly redundant
- Diversity sampling issues

```{r, fig.width= 18, fig.height=18, message = FALSE, warning = FALSE}
df %>% 
  mutate(asinsqrtMMSE = asin(sqrt(minmax(MMSE)))  ) %>%
  select(AGE, PTEDUCAT, MMSE, asinsqrtMMSE, everything(), MMSE.Change, -RID, -PTID, -EXAMDATE) %>% # removing factors with too many levels or irrelevant features
  ggpairs(aes(fill = PTGENDER, alpha = 0.7), progress = FALSE)

```
#
`marital.status` and `relationship` seem to be fairly redundant features here. (i.e. I think marital status would be a subset of the relationship category as someone who is married would necessarily be a husband or a wife). I would consider dropping the `marital.status` column.

`Education`/`education.num` are also highly redundant.  I would lean toward dropping the categorical bin variable here (as it really doesn't even reduce the possible values very much.)

I would note that both `capital.gain` and `capital.loss` are heavily skewed.  I would consider doing a log transform of these features in order to help with the skew.  

Additionally, I would guess that `capital.gain` and `capital.loss` will be relatively important features because they seem to show the cleanest separation of the two salary groups according to the scatter plots.

Moreover, because they appear to be completely orthogonal to each other.  I could combine these two features into a single variable `capital.change` where capital loss is represented as a negative value or create a binary variable `gotCapital` which indicates whether either `capital.gain` or `capital.loss` is `0`.

```{r}
df %>% 
  mutate(capitalCheck = ifelse((capital.gain > 0 & capital.loss > 0), TRUE, FALSE), orthogCheck = factor(sum(capitalCheck))) %>% str()

df %>% 
  mutate(capital.change = ifelse(capital.gain > 0, capital.gain, -capital.loss)) %>%
  filter(capital.change != 0) %>%
  ggplot(aes(x = scale(capital.change), color = salary, fill = salary)) +
  geom_bar() + 
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1)) +
  facet_wrap(~salary)

```

```{r, fig.width= 7, fig.height=7, message = FALSE, warning = FALSE}
df %>% 
  mutate(gotCapital = ifelse((capital.gain > 0 | capital.loss > 0), TRUE, FALSE)) %>%
  select(gotCapital, salary) %>%
  ggplot(aes(x = gotCapital, color = salary, fill = salary)) +
  geom_bar() + 
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1))
```

```{r, fig.height = 6, fig.width=12}
gainScatter <- df %>% 
  ggplot(aes(x = capital.gain, y = education.num, color = salary, fill = salary)) +
  geom_jitter()

logGainHist<- df %>% 
  ggplot(aes(x = log(capital.gain), color = salary, fill = salary)) +
  geom_bar() + 
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1))

plot_grid(gainScatter, logGainHist, labels = 'AUTO', ncol = 2)

```

`capital.gain`/`capital.loss` seems to be among the best features at distinctly separating people with `>50k` income. Including this as a feature could improve performance.  That said, this only accounts for a small proportion of the dataset so the improvement (if any) will likely be marginal.

Note: while I realize that doing the log removes `0` values, because the dataset is so heavily dominated by `0` (~30k), it does not improve visualization to include.

### Diversity sampling:

```{r, fig.height = 6, fig.width=12}
genderBar <- df %>% 
  ggplot(aes(x = sex, color = salary, fill = salary)) +
  geom_bar() + 
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1))

raceBar <- df %>% 
  ggplot(aes(x = race, color = salary, fill = salary)) +
  geom_bar() + 
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1))

plot_grid(genderBar, raceBar, labels = 'AUTO', ncol = 2)

```

Clearly the reported data is biased in both `sex` and `race`.

### Check for Dimensionality reduction

So far as correlation of continuous variables is concerned. Although significant, the correlation between individual variables is quite low 

Nevertheless, I try doing PCA to see if there is relative redundancy in my continuous variables. 

Ultimately there is very little condensation of variance suggesting the variables are relatively independent of each other.
```{r} 
# PCA really not too helpful
pca<- df %>% select(age, hours.per.week, fnlwgt, education.num, capital.gain, capital.loss) %>% prcomp(center = TRUE, scale = TRUE)
pca
summary(pca)

```

# Train: 

```{r}
df1 <- df %>% mutate(sqrtMMSE = sqrt(MMSE)) %>% select(-RID, -PTID, -EXAMDATE, -MMSE)

rf <- randomForest(`MMSE.Change` ~ ., data = df1, ntrees = 500, importance = TRUE, type = 'regression')
y_pred <- predict(rf)

rf
plot(rf)
importance(rf)
varImpPlot(rf)
```


```{r}
sessionInfo()
```



