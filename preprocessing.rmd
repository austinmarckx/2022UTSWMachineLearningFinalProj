---
title: "ML_Proj8 - Final"
author: "Austin Marckx"
date: "`r Sys.Date()`"
output:
  word_document: default
  pdf_document: default
  html_document: default
---

```{r setup, include=FALSE}
library(GGally)
library(ggpubr)
library(readxl)
library(randomForest)
library(caret)
library(cowplot)
library(tidyverse)
library(ggridges)
library(boot)
library(mclust)
library(corrplot)
library("FactoMineR")
library("factoextra")
library(nlme)
library(lme4)
library("REEMtree")

minmax <- function(x){
  return( (x - min(x))/(max(x) - min(x))  )
}

set.seed(3141526)
```


# Basic Preprocessing

- Read in data
- Ensure correct column types
- Check for missing data


```{r}
# Read in file
df <- read_xlsx('./data/AD.training.xlsx')

# Transform datatypes
df <- df %>% transform(
  PTID = factor(PTID),
  DX = factor(DX.bl, levels = c("CN", "EMCI", "LMCI", "AD")),
  PTGENDER = factor(PTGENDER),
  APOE4 = factor(APOE4)
) %>% select(AGE, PTEDUCAT, MMSE, PTGENDER, APOE4, DX, everything(), -DX.bl)

# Summary
df %>% head()
df %>% str()
df %>% summary()

any(is.na(df))
df[!complete.cases(df),]

```

There do not appear to be any missing values. Moreover each patient appears to be only represented once in the dataset (as the number of unique patient ids == number of rows.)


### Pair plot

- Looking for highly skewed variables
- Features which are collinear/linear transforms of one another/highly redundant
- Diversity sampling issues

```{r, fig.width= 18, fig.height=18, message = FALSE, warning = FALSE}
df %>% 
  mutate(asinsqrtMMSE = asin(sqrt(minmax(MMSE)))  ) %>%
  select(AGE, PTEDUCAT, MMSE, asinsqrtMMSE, everything(), MMSE.Change, -RID, -PTID, -EXAMDATE) %>% # removing factors with too many levels or irrelevant features
  ggpairs(aes(fill = PTGENDER, alpha = 0.7), progress = FALSE)

```
#
```{r}
# https://www.datanovia.com/en/blog/elegant-visualization-of-density-distribution-in-r-using-ridgeline/
df %>%
  ggplot(aes(x = `MMSE.Change`, y = DX, group = DX, fill = factor(stat(quantile)))) + 
  stat_density_ridges(
    geom = "density_ridges_gradient",
    calc_ecdf = TRUE,
    quantiles = 4,
    quantile_lines = TRUE, 
    jittered_points = TRUE,
    scale = 0.9,
    position = position_points_jitter(width = 0.05, height = 0.1),
    point_size = 1, point_alpha = 0.5, alpha = 0.7) + 
  theme_minimal() +
  scale_fill_brewer(name = 'Quartiles')
```

```{r, fig.width= 18, fig.height=18, message = FALSE, warning = FALSE}
df %>% 
  #mutate(asinsqrtMMSE = asin(sqrt(minmax(MMSE)))  ) %>%
  select(AGE, PTEDUCAT, MMSE, everything(), MMSE.Change, -RID, -PTID, -EXAMDATE) %>% # removing factors with too many levels or irrelevant features
  ggpairs(aes(fill = DX, alpha = 0.7), progress = FALSE)

```

```{r, fig.width= 12, fig.height=12, message = FALSE, warning = FALSE}
df %>% 
  mutate(asinsqrtMMSE = asin(sqrt(minmax(MMSE))), logMMSE = log(MMSE+1), MMSE4mo = MMSE + `MMSE.Change`, logMMSE4mo = log(MMSE4mo+1), asinsqrtMMSE4mo = asin(sqrt(minmax(MMSE4mo)))) %>%
  select(MMSE, asinsqrtMMSE, logMMSE, MMSE.Change, MMSE4mo, logMMSE4mo, asinsqrtMMSE4mo, DX) %>% # removing factors with too many levels or irrelevant features
  ggpairs(aes(fill = DX, alpha = 0.7), progress = FALSE)

```

#
```{r}
# https://www.datanovia.com/en/blog/elegant-visualization-of-density-distribution-in-r-using-ridgeline/
df %>%
  select(PTID, `MMSE.Change`, MMSE, DX) %>%
  mutate(MMSE4mo = MMSE + `MMSE.Change`) %>%
  select(-`MMSE.Change`) %>%
  pivot_longer(cols = c(MMSE4mo, MMSE),  names_to = 'time', values_to = 'MMSE') %>%
  ggplot(aes(x = MMSE, y = time, group = time, fill = factor(stat(quantile)))) + 
  stat_density_ridges(
    geom = "density_ridges_gradient",
    calc_ecdf = TRUE,
    quantiles = 4,
    quantile_lines = TRUE, 
    jittered_points = TRUE,
    scale = 0.9,
    position = position_points_jitter(width = 0.05, height = 0.1),
    point_size = 1, point_alpha = 0.5, alpha = 0.7) + 
  theme_minimal() +
  scale_fill_brewer(name = 'Quartiles')
```

#
```{r}
# https://www.datanovia.com/en/blog/elegant-visualization-of-density-distribution-in-r-using-ridgeline/
df %>%
  select(PTID, `MMSE.Change`, MMSE, DX) %>%
  mutate(MMSE4mo = MMSE + `MMSE.Change`) %>%
  select(-`MMSE.Change`) %>%
  pivot_longer(cols = c(MMSE4mo, MMSE),  names_to = 'time', values_to = 'MMSE') %>%
  ggplot(aes(x = MMSE, y = time, group = PTID, color = DX)) + 
    geom_jitter(width = 0.2, height = 0.075) +
    geom_line() +
    geom_violin(aes(color = NULL, fill = time, group = time), alpha = 0.15, draw_quantiles = c(0.25, 0.5, 0.75)) + 
  theme_minimal() +
  facet_wrap(~DX)
```

# Look at Summary stats
```{r}

# By Time
df %>% 
  select(PTID, `MMSE.Change`, MMSE, DX) %>%
  mutate(MMSE4mo = MMSE + `MMSE.Change`) %>%
  select(-`MMSE.Change`) %>%
  pivot_longer(cols = c(MMSE4mo,MMSE),  names_to = 'time', values_to = 'MMSE') %>%
  group_by(time) %>% rstatix::get_summary_stats()

# By time and DX
df %>% 
  select(PTID, `MMSE.Change`, MMSE, DX) %>%
  mutate(MMSE4mo = MMSE + `MMSE.Change`) %>%
  select(-`MMSE.Change`) %>%
  pivot_longer(cols = c(MMSE4mo,MMSE),  names_to = 'time', values_to = 'MMSE') %>%
  group_by(DX, time) %>% rstatix::get_summary_stats()

```


salmon - CN
green - EMCI (Early mild cognitive impairment)
blue - LMCI (Late mild cognitive impairment)
purple - AD

# Remove non predictive features
```{r}
# Subset to only useful features
df_clean <- df %>% select(-RID, -PTID, -EXAMDATE)
df_clean %>% head()

df_preds <- df %>% select(PTID, MMSE.Change)

# More subsets
X <- df_clean %>% select(-`MMSE.Change`)
df_numerical <- df_clean %>% select(-`MMSE.Change`, -PTGENDER, -APOE4, -DX)
df_categorical <- df_clean %>% select(-`MMSE.Change`, -AGE, -MMSE, -PTEDUCAT)
```

# GLM Regression

## gaussian
```{r}
gauss <- glm(`MMSE.Change` ~ ., data = df_clean, family = gaussian())
summary(gauss)
# RMSE
sqrt(mean(gauss$residuals ^ 2))
plot(gauss)

df_preds$glm <- predict(gauss)


df_preds %>%
  ggplot(aes(x = MMSE.Change, y = glm)) +
  geom_point(alpha = 0.7) +
  geom_abline(slope = 1, color = "red")

df_preds %>%
  ggplot(aes(x = MMSE.Change, y = round(glm, 0)) ) +
  geom_point() +
  geom_abline(slope = 1, color = "red")


```


# Random Forest Regression: 
```{r}
rf <- randomForest(`MMSE.Change` ~ ., data = df_clean, ntrees = 500, importance = TRUE, type = 'regression')
df_preds$rf <- predict(rf)

# RMSE
sqrt(mean(rf$mse))

rf
plot(rf)
importance(rf)
varImpPlot(rf)

df_preds %>%
  ggplot(aes(x = MMSE.Change, y = rf)) +
  geom_point(alpha = 0.7) +
  geom_abline(slope = 1, color = "red")

df_preds %>%
  ggplot(aes(x = MMSE.Change, y = round(rf, 0)) ) +
  geom_point() +
  geom_abline(slope = 1, color = "red")

```

# Linear Mixed Effects
```{r}
# Subset to only useful features
df_lme <- df %>% select(-RID, -EXAMDATE)
df_lme %>% head()

# More subsets
X_lme <- df_lme %>% select(-`MMSE.Change`)
```

```{r}
lmeMod <- lme(`MMSE.Change`~ AGE+MMSE+PTGENDER+PTEDUCAT+APOE4+DX, random = ~ 1|PTID, data = df_lme)
lmeMod
print("RMSE: ")
sqrt(mean(lmeMod$residuals ^ 2))
summary(lmeMod)

# RMSE
df_preds$lme <- predict(lmeMod)

plot(lmeMod)
```


```{r}
df_preds %>%
  ggplot(aes(x = MMSE.Change, y = lme)) +
  geom_point(alpha = 0.7) +
  geom_abline(slope = 1, color = "red")

df_preds %>%
  ggplot(aes(x = MMSE.Change, y = round(lme, 0)) ) +
  geom_point() +
  geom_abline(slope = 1, color = "red")

df_preds %>%
  ggplot(aes(x = MMSE.Change, y = lme)) +
  geom_jitter(height = 0, width = 0.1, alpha = 0.7) +
  geom_abline(slope = 1, color = "red")



```
```{r}
df_preds %>% 
  mutate(lme_rounded = round(lme, 0)) %>%
  pivot_longer(cols = c(glm, rf, lme, lme_rounded), names_to = "model", values_to = "ypred") %>%
  mutate(predresidual = MMSE.Change - ypred, model = factor(model, levels = c("rf", "glm", "lme", "lme_rounded")) ) %>%
  ggplot(aes(x = model, y = sqrt(abs(predresidual)), color = model)) + 
    geom_jitter(width = 0.3, height = 0.035, alpha = 0.7) +
    stat_summary(fun.data = mean_se, geom = "errorbar", color = "black", width= 0.2) +
    theme_minimal()
    


```


```{r}
lmeMod2 <- lme(`MMSE.Change`~ AGE+PTGENDER+PTEDUCAT+APOE4+DX*MMSE, random = ~ 1|PTID, data = df_lme)
lmeMod2
print("RMSE: ")
sqrt(mean(lmeMod2$residuals ^ 2))
summary(lmeMod2)

# RMSE
df_lme$lme2_yPred <- predict(lmeMod2)

plot(lmeMod2)
```


```{r}
df_lme %>%
  ggplot(aes(x = MMSE.Change, y = lme2_yPred)) +
  geom_point(alpha = 0.7) +
  geom_abline(slope = 1, color = "red")

df_lme %>%
  ggplot(aes(x = MMSE.Change, y = round(lme2_yPred, 0)) ) +
  geom_point() +
  geom_abline(slope = 1, color = "red")


```



# Random Forest Mixed Effects Regression: 
```{r}
rf2 <- randomForest(`MMSE.Change` ~ AGE+MMSE+PTGENDER+PTEDUCAT+APOE4+DX, random = ~ 1|PTID, data = df_lme)
df_lme$rf2_ypred <- predict(rf2)

# RMSE
sqrt(mean(rf2$mse))

rf2
plot(rf2)
importance(rf2)
varImpPlot(rf2)

df_lme %>%
  ggplot(aes(x = MMSE.Change, y = rf2_ypred)) +
  geom_point(alpha = 0.7) +
  geom_abline(slope = 1, color = "red")

df_lme %>%
  ggplot(aes(x = MMSE.Change, y = round(rf2_ypred, 0)) ) +
  geom_point() +
  geom_abline(slope = 1, color = "red")

```


# PCA 
```{r}
#http://www.sthda.com/english/articles/31-principal-component-methods-in-r-practical-guide/112-pca-principal-component-analysis-essentials/

resPCA <- PCA(df_numerical) # Note, this automatically scales the data

resPCA$eig
fviz_screeplot(resPCA)
corrplot(resPCA$var$contrib, is.corr = FALSE, col = COL1('YlGn', 20))


fviz_pca_ind(resPCA, col.ind = "contrib", 
             gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"),
             repel = TRUE)

fviz_pca_var(resPCA, col.var = "contrib", 
             gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"),
             repel = TRUE)

```



# FAMD
```{r}
# http://www.sthda.com/english/articles/31-principal-component-methods-in-r-practical-guide/115-famd-factor-analysis-of-mixed-data-in-r-essentials/

resFAMD <- FAMD(X)

resFAMD$eig
corrplot(resFAMD$var$contrib, is.corr = FALSE, col = COL1('YlGn', 20))
fviz_famd_var(resFAMD,"var",
             repel = TRUE)

fviz_mfa_ind(resFAMD, 
             habillage = "DX", # color by groups 
             #palette = c("#00AFBB", "#E7B800", "#FC4E07"),
             addEllipses = TRUE, ellipse.type = "confidence", 
             repel = TRUE # Avoid text overlapping
             ) 
```

# Bootstrapping (!?): 
```{r}
#https://www.statmethods.net/advstats/bootstrapping.html
bs <- function(formula, data, indices) {
  d <- data[indices,] # allows boot to select sample
  fit <- glm(formula, data=d, family = gaussian())
  return(coef(fit))
}

results <- boot(data = df_clean, statistic=bs, formula = `MMSE.Change` ~ ., R = 1000)

results
plot(results)
plot(results, index = 10)

boot.ci(results, index = 10)
```


# Gaussian Mixture: 
```{r, fig.height = 12, fig.width=12}
gmm <- Mclust(df_numerical)
summary(gmm)
plot(gmm, what = "BIC")
plot(gmm, what = "classification")
```






























```{r}
sessionInfo()
```



